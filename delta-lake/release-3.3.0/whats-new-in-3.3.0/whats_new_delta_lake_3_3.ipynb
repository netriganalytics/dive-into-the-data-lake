{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e4520a7-5b04-4585-9b71-53e1006b9392",
   "metadata": {},
   "source": [
    "# What's new in Delta Lake Release 3.3.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61f1c8-4bc6-4901-b404-789656bff015",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install delta-spark==3.3.0 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f95bf-96b5-4353-894b-65c23eb95265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-spark_2.12:3.3.0,io.delta:delta-iceberg_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba3a24-85b6-425c-a8c1-96e6e91dff86",
   "metadata": {},
   "source": [
    "## Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c0085-2815-4339-9979-0e0bb4f1c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"What's New in Delta Lake 3.3.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a8938-0a23-4ab5-a348-9b2a454ffa20",
   "metadata": {},
   "source": [
    "# #1 - Support for Declaring Identity Columns\n",
    "\n",
    "Identity columns are a type of generated column that assigns unique values to each record inserted into a table. Delta Lake 3.3.0, adds support for leveraging the Python or Scala `DeltaTableBuilder` class to generate unique keys for new rows.\n",
    "\n",
    "Identity columns come in two flavors: `generatedAlwaysAs` and `generatedByDefaultAs`\n",
    "\n",
    "- `generatedAlwaysAs` - automatically generates unique values for inserted records\n",
    "- `generatedByDefaultAs` - provides flexibility for the writer to specify unique values for the inserted record. If no value is specified, then a new, unique value is generated \n",
    "\n",
    "Let’s take a look at an example together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f496e7-b16a-4058-ad88-3366827a4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable, IdentityGenerator\n",
    "\n",
    "# Identity columns only support LONG data type\n",
    "DeltaTable.createOrReplace(spark) \\\n",
    "  .tableName(\"Customer\") \\\n",
    "  .addColumn(\"c_custkey\", \"LONG\", generatedByDefaultAs=IdentityGenerator(start=1, step=1)) \\\n",
    "  .addColumn(\"c_name\", \"STRING\") \\\n",
    "  .addColumn(\"c_address\", \"STRING\") \\\n",
    "  .addColumn(\"c_nationkey\", \"LONG\") \\\n",
    "  .addColumn(\"c_phone\", \"STRING\") \\\n",
    "  .addColumn(\"c_acctbal\", \"DOUBLE\") \\\n",
    "  .addColumn(\"c_mktsegment\", \"STRING\") \\\n",
    "  .addColumn(\"c_comment\", \"STRING\") \\\n",
    ".execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb4694-0392-4bbb-81de-f2febb47cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a few rows\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO Customer (c_name, c_address, c_nationkey, c_phone, c_acctbal, c_mktsegment, c_comment)\n",
    "VALUES\n",
    "   ('ACME, Inc.', '123 Fake Street', 23, '12-475-733-1633', 523.10, 'MACHINERY', 'run about the final theodolites. blithely unusual dolphins are furio'),\n",
    "   ('Just Parts, LLC', '123 Pines Lane', 23, '11-461-373-7563', 1220.45, 'AUTOMOBILE', 'ges doze according to the carefully pending'),\n",
    "   ('Wing Nuts & More, Inc.', '123 Fake Avenue', 23, '32-136-289-9352', 6324.46, 'BUILDING', 'jole slyly unusual deposits. furiously ironic requests cajole blithely'),\n",
    "   ('Build Stuff, Inc.', '123 Fake Court', 23, '17-263-616-2325', 9296.71, 'BUILDING', 'leep carefully slyly express sentiments. slyly pending instructions wake above the carefully'),\n",
    "   ('Munchin Machines', '123 Meadow Lane', 23, '24-339-805-7967', 8362.10, 'MACHINERY', 'quickly. carefully regular requests cajole above the special, pending pa')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee326da-8433-416d-8c1a-cc77bd87431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With `generatedByDefaultAs`, I can choose to provide a value for the id col\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO Customer (c_custkey, c_name, c_address, c_nationkey, c_phone, c_acctbal, c_mktsegment, c_comment)\n",
    "VALUES\n",
    "   (1206, 'Just Wheels, LLC', '123 Fake ST', 23, '22-353-733-1783', 7683.10, 'AUTOMOBILE', 'about carefully slyly express sentiments. blithely unusual')\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe48540-3b01-4acd-80ab-6d37c666be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek!\n",
    "display(spark.table(\"Customer\").toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b617d07-b548-4541-a4fd-400208340ec6",
   "metadata": {},
   "source": [
    "# #2 - Faster Table Vacuums using VACUUM LITE\n",
    "This release introduces 2 new vacuum types: `FULL` and `LITE`.\n",
    "\n",
    "- `VACUUM FULL` (default) - will recursively list all the table files and subdirectories for a table’s root path\n",
    "- `VACUUM LITE` - calculate eligible files to be deleted by looking at the earliest commit and latest commit outside of the retention window (`RETAIN N HOURS` clause) in the Delta transaction log\n",
    "\n",
    "`VACUUM LITE` doesn’t replace `VACUUM FULL` entirely. Keep in mind, Delta transaction log files are deleted automatically after each log checkpoint operation (see `delta.logRetentionDuration`), so there still the possibility that some old table files fall through the cracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11496456-1132-4206-af8a-28f9e8f14218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling this to remove all the table history (do NOT do this in production!)\n",
    "# _Note_: we just created this table so there's not much to clean up!\n",
    "# Try disabling retention check and setting to the retention window to 0\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\n",
    "spark.conf.set(\"delta.deletedFileRetentionDuration\", \"interval 0 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b29f1-fc84-48ac-9bf4-049b833f7f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Lake 3.3.0 introduces vacuum types `LITE` and `FULL`\n",
    "spark.sql(\"\"\"\n",
    "VACUUM Customer\n",
    "   LITE\n",
    "   RETAIN 240 HOURS   \n",
    "   DRY RUN\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d66475-2c73-4ec8-a79d-25a78c25e8e6",
   "metadata": {},
   "source": [
    "# #3 - Ehanced Support for Row Tracking\n",
    "\n",
    "Delta Lake 3.3.0 adds support for backfilling an existing Delta table with unique row identifiers.\n",
    "\n",
    "You can enable row tracking by setting the table property `delta.enableRowTracking`. \n",
    "\n",
    "For example:\n",
    "\n",
    "```ALTER TABLE my_table SET TBLPROPERTIES ('delta.enableRowTracking' = 'true');```\n",
    "\n",
    "Row tracking adds two new **metadata** fields to a Delta table, when enabled:\n",
    "\n",
    "- `_metadata.row_id` - a unique identifier of the row\n",
    "- `_metadata.row_commit_version` - table version at which the row was last inserted or updated\n",
    "\n",
    "By default, these metadata fields are hidden but can be read using the `_metadata` column.\n",
    "\n",
    "This release adds support for altering existing Delta tables, enabling row-tracking. Behind the scenes, a backfill process is triggered which will populate existing table rows with unique row IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600f80e-771e-49d0-810a-43fc2638516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE Customer\n",
    "SET TBLPROPERTIES (\n",
    "   'delta.enableRowTracking' = 'true'\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0732a0-4e93-4a34-a10a-79e0073b140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "   _metadata.row_id, \n",
    "   _metadata.row_commit_version,\n",
    "   * \n",
    "FROM\n",
    "   Customer\n",
    "ORDER BY _metadata.row_id ASC;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4853f-b123-424b-8d83-bd685b76c23f",
   "metadata": {},
   "source": [
    "Now that row tracking is enabled on our Delta table, let's merge a new record and update an existing record to generate some table history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0a293-1eff-4d6c-8637-511a2bcb43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a few new customers\n",
    "new_customers = [\n",
    "    # An update to `c_name`\n",
    "    (5, 'Machine Parts', '123 Meadow Lane', 23, '24-339-805-7967', 8362.10, 'MACHINERY', 'quickly. carefully regular requests cajole above the special, pending pa'),\n",
    "    (1207, 'Sockets & Rockets', '4456 Swim Lane', 23, '14-292-888-5532', 342.45, 'MACHINERY', 'slyly unusual deposits. furiously ironic requ, pending special things'),\n",
    "]\n",
    "df = spark.createDataFrame(data=new_customers, schema=\"c_custkey LONG, c_name STRING, c_address STRING, c_nationkey LONG, c_phone STRING, c_acctbal DOUBLE, c_mktsegment STRING, c_comment STRING\")\n",
    "df.createOrReplaceTempView(\"new_customers_vw\")\n",
    "\n",
    "# MERGE new customers into the existing Delta table\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO Customer t\n",
    "USING new_customers_vw s\n",
    "ON t.c_custkey = s.c_custkey\n",
    "WHEN MATCHED\n",
    "   THEN UPDATE SET\n",
    "      c_name = s.c_name,\n",
    "      c_address = s.c_address,\n",
    "      c_nationkey = s.c_nationkey,\n",
    "      c_phone = s.c_phone,\n",
    "      c_acctbal = s.c_acctbal,\n",
    "      c_mktsegment = s.c_mktsegment,\n",
    "      c_comment = s.c_comment\n",
    "WHEN NOT MATCHED\n",
    "   THEN INSERT (\n",
    "      c_custkey,\n",
    "      c_name,\n",
    "      c_address,\n",
    "      c_nationkey,\n",
    "      c_phone,\n",
    "      c_acctbal,\n",
    "      c_mktsegment,\n",
    "      c_comment\n",
    "   ) VALUES (\n",
    "      s.c_custkey,\n",
    "      s.c_name,\n",
    "      s.c_address,\n",
    "      s.c_nationkey,\n",
    "      s.c_phone,\n",
    "      s.c_acctbal,\n",
    "      s.c_mktsegment,\n",
    "      s.c_comment\n",
    "   )\n",
    "\"\"\")\n",
    "\n",
    "spark.table(\"Customer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9747a-49e2-40f4-9b79-523e17b668de",
   "metadata": {},
   "source": [
    "## Tracking row-level lineage\n",
    "\n",
    "When row-tracking is enabled, users can identify rows across multiple versions of the table using Delta's time travel feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c84db0-bd19-4683-9121-b46a7530fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"\"\"\n",
    "(SELECT '5' AS `version`, *\n",
    "  FROM Customer\n",
    "VERSION AS OF 5\n",
    "WHERE _metadata.row_id = 4)\n",
    "UNION ALL\n",
    "(SELECT '6' as `version`, *\n",
    "  FROM Customer\n",
    "VERSION AS OF 6\n",
    "WHERE _metadata.row_id = 4)\n",
    "\"\"\").toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98570b49-46de-4d44-9ede-49a50a86676b",
   "metadata": {},
   "source": [
    "# #4 - Support for Fully Re-clusterting a Liquid Table\n",
    "\n",
    "This release adds support for enabling liquid clustering on an existing Delta table.\n",
    "\n",
    "Previously, liquid clustering could only be enabled upon table creation.\n",
    "\n",
    "Let's look at how easy it is to enable liquid clustering on our existing, unpartitioned Delta table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4316510-4902-4a8f-8a0b-d111cdfbd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE Customer\n",
    "CLUSTER BY (c_mktsegment)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5729a-fa56-4954-aceb-699609b21277",
   "metadata": {},
   "source": [
    "This release adds support for fully re-clustering a Delta table by new columns using an `OPTIMIZE FULL` command. \n",
    "\n",
    "`OPTIMIZE FULL` will optimize all records in a Delta table that uses liquid clustering, including data that might have been previously clustered by other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f1619-febb-4b3b-b981-5844afb25f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"\"\"\n",
    "OPTIMIZE Customer FULL\n",
    "\"\"\").toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61cb46-3006-4e42-8c7b-a492587cd35e",
   "metadata": {},
   "source": [
    "# #5 - Enable UniForm Iceberg In-place\n",
    "Previously, enabling UniForm Iceberg on an existing Delta table meant that the table's data files would need to be rewritten along with an Iceberg metadata layer. \n",
    "\n",
    "This release supports enabling UniForm Iceberg on a Delta table in place, meaning that the data layer doesn't need to be rewritten. \n",
    "\n",
    "Using an `ALTER TABLE` statement, you can enable UniForm Iceberg on an existing Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f9006-1ead-4c97-832c-66a7186e18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TPC-H Region table\n",
    "DeltaTable.createOrReplace(spark) \\\n",
    "  .tableName(\"Region\") \\\n",
    "  .addColumn(\"r_regionkey\", \"LONG\") \\\n",
    "  .addColumn(\"r_name\", \"STRING\") \\\n",
    "  .addColumn(\"r_comment\", \"STRING\") \\\n",
    ".execute()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO Region\n",
    "VALUES (23, 'EUROPE', 'hs use ironic, even requests. s'),\n",
    "       (24, 'ASIA', 'uickly special accounts cajole carefully blithely close requests. carefully final asymptotes haggle furiousl'),\n",
    "       (25, 'AFRICA', 'lar deposits. blithely final packages cajole. regular waters are final requests. regular accounts are according to '),\n",
    "       (26, 'MIDDLE EAST', 'ges. thinly even pinto beans cages. thinly even pinto beans ca'),\n",
    "       (27, 'AMERICA', 'lithely final packages cajole. regular waters are final requests. regular ')\n",
    "\"\"\")\n",
    "\n",
    "display(spark.table(\"Region\").toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b4702-cad5-4cb0-942f-bf0da67c19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniForm requires column mapping to be enabled on the Delta table\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE Region\n",
    "SET TBLPROPERTIES(\n",
    "  'delta.columnMapping.mode' = 'name'\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7064870-b95f-4f34-9d25-9f5e5f10d9fe",
   "metadata": {},
   "source": [
    "## Important Note\n",
    "\n",
    "You will need a **Hive Metastore** to execute the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6b341-c621-47ab-ad14-9cdb7b0b6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE Region\n",
    "SET TBLPROPERTIES(\n",
    "  'delta.enableIcebergCompatV2' = 'true',\n",
    "  'delta.universalFormat.enabledFormats' = 'iceberg'\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499562f-aee4-4b88-81cf-2738df652e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
